# train.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from src.config import Config
from src.data.dataset import SudokuDataset
from src.model.transformer import SudokuTransformer
from src.utils import calculate_accuracy, seed_everything

def main():
    seed_everything(42)
    print(f"π”§ ν•™μµ μ¥μΉ: {Config.DEVICE}")
    
    # λ¨λΈ μ €μ¥ ν΄λ” μƒμ„±
    if not os.path.exists(Config.MODEL_SAVE_DIR):
        os.makedirs(Config.MODEL_SAVE_DIR)
    
    # λ°μ΄ν„° λ΅λ“
    train_loader = DataLoader(SudokuDataset(f"{Config.DATA_DIR}/train.pt"), 
                            batch_size=Config.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(SudokuDataset(f"{Config.DATA_DIR}/val.pt"), 
                          batch_size=Config.BATCH_SIZE, shuffle=False)
    
    model = SudokuTransformer(Config).to(Config.DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=Config.LR)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.EPOCHS)
    
    # --- [λ³µκµ¬λ¨] μ²΄ν¬ν¬μΈνΈ λ΅λ“ λ΅μ§ ---
    start_epoch = 0
    best_acc = 0.0
    checkpoint_path = f"{Config.MODEL_SAVE_DIR}/last_checkpoint.pth"

    if os.path.exists(checkpoint_path):
        print(f"π”„ μ²΄ν¬ν¬μΈνΈ λ°κ²¬! ν•™μµμ„ μ¬κ°ν•©λ‹λ‹¤: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=Config.DEVICE)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_acc = checkpoint['best_acc']
            print(f"   β–¶ Epoch {start_epoch+1}λ¶€ν„° μ‹μ‘ν•©λ‹λ‹¤. (ν„μ¬ μµκ³  κΈ°λ΅: {best_acc*100:.2f}%)")
        except Exception as e:
            print(f"β οΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨ (νμΌ κΉ¨μ§ λ“±): {e}")
            print("   β¨ μ²μλ¶€ν„° λ‹¤μ‹ μ‹μ‘ν•©λ‹λ‹¤.")
    else:
        print("β¨ μ²μλ¶€ν„° ν•™μµμ„ μ‹μ‘ν•©λ‹λ‹¤.")

    # ν•™μµ λ£¨ν”„
    for epoch in range(start_epoch, Config.EPOCHS):
        model.train()
        train_loss = 0
        train_acc = 0
        
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS}")
        
        for p, s in loop:
            p, s = p.to(Config.DEVICE), s.to(Config.DEVICE)
            
            optimizer.zero_grad()
            out = model(p)
            loss = criterion(out.view(-1, Config.NUM_CLASSES), s.view(-1))
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_acc += calculate_accuracy(out, s)
            
            loop.set_postfix(loss=loss.item())

        scheduler.step()
        
        # κ²€μ¦
        model.eval()
        val_acc = sum([calculate_accuracy(model(p.to(Config.DEVICE)), s.to(Config.DEVICE)) for p, s in val_loader]) / len(val_loader)
        
        current_lr = scheduler.get_last_lr()[0]
        print(f"   Done! Val Acc: {val_acc*100:.2f}% | LR: {current_lr:.6f}")
        
        # --- μ²΄ν¬ν¬μΈνΈ μ €μ¥ (λ§¤ μ—ν­λ§λ‹¤) ---
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_acc': best_acc
        }, checkpoint_path)

        # μµκ³  κΈ°λ΅ μ €μ¥
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), Config.MODEL_PATH)
            print(f"   π† μµκ³  κΈ°λ΅ κ²½μ‹ ! λ¨λΈ μ €μ¥λ¨: {Config.MODEL_PATH}")

if __name__ == "__main__":
    main()